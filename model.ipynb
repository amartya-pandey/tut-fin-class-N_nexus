{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DataLoader:\n",
    "    def load_documents(self, directory):\n",
    "        documents = []\n",
    "        for filename in os.listdir(directory):\n",
    "            with open(os.path.join(directory, filename), 'r', encoding='utf-8') as file:\n",
    "                text = file.read()\n",
    "                documents.append((filename, text))\n",
    "        return documents\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextPreprocessor:\n",
    "    def __init__(self):\n",
    "        self.stemmer = PorterStemmer()\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    def clean_text(self, text):\n",
    "        text = re.sub(r'\\W', ' ', text)  # Remove special characters\n",
    "        text = text.lower()  # Convert to lowercase\n",
    "        return text\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        tokens = text.split()\n",
    "        return [self.stem(token) for token in tokens if token not in self.stop_words]\n",
    "\n",
    "    def stem(self, word):\n",
    "        return self.stemmer.stem(word)\n",
    "\n",
    "    def preprocess_documents(self, documents):\n",
    "        processed_docs = []\n",
    "        for filename, text in documents:\n",
    "            cleaned_text = self.clean_text(text)\n",
    "            tokens = self.tokenize(cleaned_text)\n",
    "            processed_docs.append((filename, tokens))\n",
    "        return processed_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vectorizer:\n",
    "    def __init__(self):\n",
    "        self.vectorizer = TfidfVectorizer()\n",
    "\n",
    "    def fit_transform(self, texts):\n",
    "        return self.vectorizer.fit_transform(texts)\n",
    "\n",
    "    def transform(self, texts):\n",
    "        return self.vectorizer.transform(texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self):\n",
    "        self.model = LogisticRegression(max_iter=1000)\n",
    "\n",
    "    def train(self, X, y):\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        self.model.fit(self.X_train, self.y_train)\n",
    "\n",
    "    def evaluate(self):\n",
    "        predictions = self.model.predict(self.X_test)\n",
    "        print(classification_report(self.y_test, predictions))\n",
    "\n",
    "    def tune_hyperparameters(self, param_grid):\n",
    "        grid_search = GridSearchCV(self.model, param_grid, cv=5)\n",
    "        grid_search.fit(self.X_train, self.y_train)\n",
    "        print(\"Best parameters found: \", grid_search.best_params_)\n",
    "        return grid_search.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EDA:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def visualize_data_distribution(self):\n",
    "        seaborn.countplot(y='category', data=self.data)\n",
    "        plt.title('Distribution of Article Categories')\n",
    "        plt.show()\n",
    "\n",
    "    def calculate_statistics(self):\n",
    "        lengths = self.data['text'].apply(lambda x: len(x.split()))\n",
    "        print(f'Mean length of articles: {lengths.mean()}')\n",
    "        print(f'Median length of articles: {lengths.median()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bootstrapping:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def retrain_on_misclassified(self, X_test, y_test):\n",
    "        predictions = self.model.predict(X_test)\n",
    "        misclassified_indices = [i for i, (pred, true) in enumerate(zip(predictions, y_test)) if pred != true]\n",
    "        \n",
    "        if misclassified_indices:\n",
    "            X_misclassified = X_test[misclassified_indices]\n",
    "            y_misclassified = y_test[misclassified_indices]\n",
    "            self.model.fit(X_misclassified, y_misclassified)\n",
    "            print(\"Model retrained on misclassified articles.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "# Load Data\n",
    "    loader = DataLoader()\n",
    "    documents = loader.load_documents('data/your_financial_articles_directory/')\n",
    "    \n",
    "# Preprocessing\n",
    "    preprocessor = TextPreprocessor()\n",
    "    processed_documents = preprocessor.preprocess_documents(documents)\n",
    "    \n",
    "# Creating a DataFrame for unlabeled data\n",
    "    data = pd.DataFrame(processed_documents, columns=['filename', 'tokens'])\n",
    "    data['text'] = data['tokens'].apply(lambda x: ' '.join(x))  # Join tokens back to text\n",
    "\n",
    "# Load labeled categories\n",
    "    categories_df = pd.read_csv('data/categories.csv')\n",
    "\n",
    "# Vectorization\n",
    "    vectorizer = Vectorizer()\n",
    "    X_labeled = vectorizer.fit_transform(categories_df['text'])\n",
    "    y_labeled = categories_df['category']\n",
    "\n",
    "# Train the model on the labeled data\n",
    "    model = Model()\n",
    "    model.train(X_labeled, y_labeled)\n",
    "\n",
    "# Predict categories for the unlabeled dataset\n",
    "    X_unlabeled = vectorizer.transform(data['text'])\n",
    "    predictions = model.model.predict(X_unlabeled)\n",
    "\n",
    "# Add predictions to the DataFrame\n",
    "    data['category'] = predictions\n",
    "\n",
    "# Display predictions to check results\n",
    "    print(data[['filename', 'category']])\n",
    "\n",
    "# Check distribution of predicted categories\n",
    "    print(\"Predicted category distribution:\")\n",
    "    print(data['category'].value_counts())\n",
    "\n",
    "# Exploratory Data Analysis\n",
    "    eda = EDA(data)\n",
    "    eda.visualize_data_distribution()\n",
    "    eda.calculate_statistics()\n",
    "\n",
    "# Evaluate model performance (optional)\n",
    "    model.evaluate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
